<H1>Hi there ðŸ‘‹</H1>

I am Dengyun Peng, a **first-year Master's student** at **HIT** and a member of the **SCIR LA**.
I am currently under the supervision of Professor Wanxiang Che, Professor Libo Qin and Ph.D. candidate Qiguang Chen.
My current research interests focus on **RL4LLM**, **LLM reasoning**. I have research experience in **Safe RL** and **Offline RL**.

<H1>Intern:</H1>

- **iFLYTEK** (Hefei)
  - _Research Intern_, September 2025 â€“ Present

- **Du Xiaoman Financial** (Beijing)
  - _Research Intern_, January 2025 â€“ February 2025
 
- **Westlake University** (Hangzhou)
  - _Research Intern_, December 2023 â€“ September 2024
    
<H1>Publication:</H1>

(EMNLP2025 Findings, Co-First author) DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization Framework from a Deep-Learning Perspective (https://arxiv.org/abs/2503.13413)

(NIPS2025, Co-First author) Boundary-to-Region Supervision for Offline Safe Reinforcement Learning (https://nips.cc/virtual/2025/poster/115428)

(AAAI2026, Co-First author) Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models (https://arxiv.org/abs/2508.11582)

(ICML2024, Second author) Reinformer: Max-Return Sequence Modeling for Offline RL (https://proceedings.mlr.press/v235/zhuang24b.html)

(SCIENCE CHINA Information Sciences, Fourth Author) Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models (https://arxiv.org/abs/2503.09567)

(Preprint, Fourth Author) ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model (https://arxiv.org/abs/2502.03325)

<H1>Email:</H1>

dypeng@ir.hit.edu.cn

pengdengyun@qq.com

<H1>Google scholar</H1>

https://scholar.google.com.hk/citations?user=XtG_SxwAAAAJ&hl=zh-CN
